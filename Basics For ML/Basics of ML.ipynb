{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb100615",
   "metadata": {},
   "source": [
    "## Basics For ML :-> \n",
    "\n",
    "<b> What is Artificial Intelligence ?</b> \n",
    "\n",
    "    Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision. Artificial intelligence is a theory and development of computer systems that can perform tasks that normally require human intelligence.\n",
    "    \n",
    "<b> What is Machine Learning ?</b> \n",
    "\n",
    "    Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Machine Learning helps us to develop computer systems that are able to learn and adapt without following explicit instructions, by using algorithms and statistical models to analyse and draw inferences from patterns in data.\n",
    "    \n",
    "<b> What is Deep Learning ?</b> \n",
    "\n",
    "    Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data. Similarly to how we learn from experience, the deep learning algorithm would perform a task repeatedly, each time tweaking it a little to improve the outcome.\n",
    "    \n",
    "<img src = \"images/60f6feb4be651f666b46194a_AI vs Machine Learning vs Deep Learning.jpeg\" alt = \"difference between AI/ML/DL\" width = 500 height = 500/>\n",
    "\n",
    "<b> What are differnet types of Machine Learning ?</b>\n",
    "\n",
    "There are primarily three types of machine learning: Supervised, Unsupervised, and Reinforcement Learning.\n",
    "\n",
    "    1) Supervised Learning :- Supervised learning is a type of machine learning that uses labeled data to train machine learning models. In labeled data, the output is already known. The model just needs to map the inputs to the respective outputs. \n",
    "\n",
    "    An example of supervised learning is to train a system that identifies the image of an animal. \n",
    "\n",
    "    2) Unsupervised Learning :- Unsupervised learning is a type of machine learning that uses unlabeled data to train machines. Unlabeled data doesn’t have a fixed output variable. The model learns from the data, discovers the patterns and features in the data, and returns the output. \n",
    "    An example of unsupervised learning is such as grouping customers by purchasing behavior.\n",
    "    \n",
    "    3) Reinforcement Learning :- Reinforcement Learning trains a machine to take suitable actions and maximize its rewards in a particular situation. It uses an agent and an environment to produce actions and rewards. The agent has a start and an end state. But, there might be different paths for reaching the end state, like a maze. In this learning technique, there is no predefined target variable. \n",
    "    An example of reinforcement learning is to train a machine that can identify the shape of an object, given a list of different objects.\n",
    "   \n",
    "<b> Different kinds of Machine Learning Tasks :- </b>\n",
    "    \n",
    "All of the Machine Learning algorithms take data as input, but what they want to achieve is different.\n",
    "\n",
    "They can be broadly be classified in a few groups based on the task they are designed to solve. These tasks are: classification, regression and clustering.\n",
    "\n",
    "    1) Classification :- Classification is a supervised learning task used when we need a limited set of outcomes. It generally provides predicted output values, which may be  True or False.  It performs in the way of two types such as Binomial and Multi-Class. For example, we can find whether a received email is Spam or Not.  \n",
    "    \n",
    "    2) Regression :- The regression task comes from Supervised machine learning. Which can help us to predict (expect continues values) and explains objects based on a given set of numerical and categorical data. For example, we can predict house prices based on house attributes such as several rooms, size, and location.\n",
    "    \n",
    "    3) Clustering :- An unsupervised learning technique performs as a way of grouping related items together into various clusters. It doesn’t have any output information for the training process. Nonetheless, the clustering algorithm will define the output.\n",
    "    \n",
    "    4) Dimensionality Reduction :- An unsupervised learning task used to reduce redundant information from large data sets. It provides less computation and reduces training time.\n",
    "    \n",
    "<br>\n",
    "<img src = \"images/maxresdefault.jpeg\" alt = \"classification/regression\" width = 800 height = 800/>\n",
    "<br>\n",
    "\n",
    "Now we will learn about different kind of datasets we uses in Machine Learning.\n",
    "\n",
    "<b>1) Training Data :-</b> The sample of data used to fit the model. The actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "\n",
    "<b>2) Validation Data :- </b>The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "<b>3) Testing Data :- </b> The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "<b> Validation set actually can be regarded as a part of training set, because it is used to build your model. It is usually used for parameter selection and to avoild overfitting.</b>\n",
    "\n",
    "### Different kind of Distance Measures used in ML :->\n",
    "\n",
    "    1) Euclidean Distance :- Euclidean distance is the distance between two points. To find the two points on a plane, the length of a segment connecting the two points is measured.\n",
    "    \n",
    "<br>\n",
    "<img src = \"images/formula-of-euclidean-distance-1624039148.png\" alt = \"Euclidean formula\" width = 300 height = 300/>\n",
    "<br>\n",
    "\n",
    "The Euclidean distance formula says:\n",
    "\n",
    "$$\n",
    "    d = \\sqrt{[ (x_{2} – x_{1})^2 + (y_{2} – y_{1})^2]}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text {where, }\\\\\n",
    "(x_{1}, y_{1})\\text { -  are the coordinates of one point., }\\\\\n",
    "(x_{2}, y_{2})\\text { -  are the coordinates of the other point., }\\\\\n",
    "d \\text { -  is the distance between (x1, y1) and (x2, y2). }\\\\\n",
    "$\n",
    "\n",
    "$$ d = ||A-B|| == \\text {length of distance between A and B, }\n",
    "$$\n",
    "\n",
    "$\\text {Lets say, } $\n",
    "$$ A \\in R^d and  B \\in R^d \\text {    then, }\\\\\n",
    "||A-B||_2 = [\\sum\\limits_{i=0}^{d}(A_i - B_i)^2]^\\frac{1}{2} $$\n",
    "\n",
    "$ \\text {                   this is the L2 norm, } $\n",
    "\n",
    "<br>\n",
    "\n",
    "    2) Manhattan distance :- Manhattan distance is calculated as the sum of the absolute differences between the two vectors.\n",
    "\n",
    "<b>The Manhattan distance formula says: lets suppose there are 2 points A and B in 2 dimensional space, and coordinates of point A is (x1,y1) and coordinates of point B is (x2,y2) then </b>\n",
    "\n",
    "$$\n",
    "    d = | (x_{2} – x_{1})| + |(y_{2} – y_{1})|\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text {where, }\\\\\n",
    "(x_{1}, y_{1})\\text { -  are the coordinates of one point., }\\\\\n",
    "(x_{2}, y_{2})\\text { -  are the coordinates of the other point., }\\\\\n",
    "d \\text { -  is the distance between (x1, y1) and (x2, y2). }\\\\\n",
    "$\n",
    "\n",
    "$$ d = ||A-B|| == \\text {length of distance between A and B, }\n",
    "$$\n",
    "\n",
    "$\\text {Lets say, } $\n",
    "$$ A \\in R^d and  B \\in R^d \\text {    then, }\\\\\n",
    "||A-B||_1 = [\\sum\\limits_{i=0}^{d}|(A_i - B_i)|^\\frac{1}{1} $$\n",
    "\n",
    "$ \\text {                   this is the L1 norm, } $\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src = \"images/0__9ljPf7RbVI5cVdG.png\" alt = \"difference betwwen euc. and manhattan\" />\n",
    "<br>\n",
    "\n",
    "    3) Minkowski Distance :- Minkowski distance calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated.\n",
    "<b> Minkowski Distance is also called the $L_p$ norm of the vector and is calculated by given formula :-</b>\n",
    "\n",
    "Lets suppose there are 2 points A and B in d dimensional space, then \n",
    "\n",
    "<!-- $$\n",
    "    d = | (x_{2} – x_{1})| + |(y_{2} – y_{1})|\n",
    "$$ -->\n",
    "\n",
    "$$ d = ||A-B|| == \\text {length of distance between A and B, }\n",
    "$$\n",
    "\n",
    "$\\text {Lets say, } $\n",
    "$$ A \\in R^d and  B \\in R^d \\text {    then, }\\\\\n",
    "||A-B||_p = [\\sum\\limits_{i=0}^{d}|(A_i - B_i)|^p]^\\frac{1}{p} $$\n",
    "\n",
    "$ \\text {                   this is the L1 norm, } $\n",
    "\n",
    "$\n",
    "\\text {where, }\\\\\n",
    "(x_{1}, y_{1})\\text { -  are the coordinates of one point., }\\\\\n",
    "(x_{2}, y_{2})\\text { -  are the coordinates of the other point., }\\\\\n",
    "d \\text { -  is the distance between A and B. }\\\\\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "<img src = \"images/minowski-distance.png\" alt = \"Minkowski distance\" />\n",
    "<br>\n",
    "\n",
    "<b> Note :- distance is always between two points and norms are always for a vector</b>\n",
    "<br>\n",
    "\n",
    "    4) Hamming Distance :- Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short. You are most likely going to encounter bitstrings when you one-hot encode categorical columns of data. It is mainly used in text preprocessing.\n",
    "    \n",
    "$ \\text{Hamming distance}(x_1,x_2) = (\\text{ number of lacations where binary vectors differs}) $\n",
    "<br>\n",
    "<br>\n",
    "<img src = \"images/Example-of-Hamming-Distance.png\" alt = \"hamming distance\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Cosine Similarity and Cosine Distance :- \n",
    "    \n",
    "    1) Cosine Similarity :- Cosine similarity is used to determine the similarity between vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. Cosine similarity says that to find the similarity between two points or vectors we need to find Angle between them.\n",
    "    \n",
    "    2) Cosine Distance :- Cosine distance is calculated as 1 - cosine similarity. Here A=Point P1,B=Point P2\n",
    "<br>\n",
    "<img src = \"images/1_CHTUjtaensX3H8pxh9lYLQ.png\" alt = \"cosine similarity formula\"/>\n",
    "<br>\n",
    "\n",
    "<b> Note :-  cosine similarity and cosine distance are inversely proportional to each other. That means, if similarity increases, distance decreases, and if similarity decreases then distance increases. </b>\n",
    "<br>\n",
    "\n",
    "<span style=\"color:green\"> <b>The cosine similarity is a number between 0 and 1 and thus cosine distance ranges from 0 to 2.</b></span>\n",
    "\n",
    "<br>\n",
    "<img src = \"images/WhatsApp Image 2022-06-06 at 8.11.50 PM.jpeg\" alt = \"cosine distance\" width = 400 height = 400/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c9fa0",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting :- \n",
    "    \n",
    "    1) Overfitting :- If the model try to fit a function or decision surface such that it tries to make no errors or mistake but thr decision surface is extremmely non smooth such a case is called Overfitting. \n",
    "    \n",
    "    Good performance on the training data, poor generliazation to other data. The main reason overfitting happens is because you have a small dataset and you try to learn from it. The algorithm will have greater control over this small dataset and it will make sure it satisfies all the datapoints exactly.\n",
    "    \n",
    "    2) Underfitting :- Underfitting says i dont care about data points, it will just classify new points according to majority of class in training data. \n",
    "    \n",
    "    Poor performance on the training data and poor generalization to other data. Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset.\n",
    "    \n",
    "<br>\n",
    "<img src = \"images/download.jpeg\" alt= \"overfitting and underfitting\" width = 400 hight = 400/>\n",
    "<br>\n",
    "<b> From the figure we can see that underfitting has both error high (training and testing error) while overfitting has low training error and high testing error.</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Cross Validation :- \n",
    "\n",
    "    Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set. It is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model.\n",
    "    In cross validation we split data into 3 parts - training, testing and validating data.\n",
    "\n",
    "<b> Why do we need Validation Data ?</b>\n",
    "\n",
    "    We often randomly split the dataset into train data and test data to develop a machine learning model. The training data is used to train the ML model and the same model is tested on independent testing data to evaluate the performance of the model.\n",
    "    With the change in the random state of the split, the accuracy of the model also changes, so we are not able to achieve a fixed accuracy for the model. The testing data should be kept independent of the training data so that no data leakage occurs. During the development of an ML model using the training data, the model performance needs to be evaluated. Hence the importance of cross-validation data comes into the picture.\n",
    "    \n",
    "Lets suppose we split data in 3 parts :- \n",
    "\n",
    "$$ \n",
    "    D_{train} = 60\\% \\text{ of } D_n \\\\\n",
    "    D_{validation} = 20\\% \\text{ of } D_n \\\\\n",
    "    D_{test} = 20\\% \\text{ of } D_n\n",
    "$$\n",
    "\n",
    "$ \\text {Where, }\\\\\n",
    "D_{train} \\text{ - training data, } \\\\\n",
    "D_{validation} \\text{ - validating data, } \\\\\n",
    "D_{test}  \\text{ - testing data }\n",
    "$\n",
    "\n",
    "Now then we trained our model on traing data, and validated performance on validation data, but there is one problem here, in this approach <b> we are lossing 20% training data, so we have to find a way such that we can use 80% of data for training and still we have 20% validation data with 20% testing data.</b>\n",
    "\n",
    "<b> K-fold Cross Validation :- </b> In this method, we split the data-set into k number of subsets(known as folds) then we perform training on the all the subsets but leave one(k-1) subset for the evaluation of the trained model. In this method, we iterate k times with a different subset reserved for testing purpose each time. \n",
    " \n",
    "<br>\n",
    "<img src = \"images/cross_validation.png\" alt= \"overfitting and underfitting\" width = 400 hight = 400/>\n",
    "<br>\n",
    "\n",
    "<b> In this method, first we calculate accuracy of each fold and then take average of it as training accuracy.</b>\n",
    "\n",
    "<span style=\"color:green\"><b>Here K is hyper-parameter, in general we take k as 5 or 10. And also training time is increased by K times.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca9c9d",
   "metadata": {},
   "source": [
    "### Real World Cases :-\n",
    "\n",
    "<b>1) Balanced Vs. Imbalanced Dataset :-</b> \n",
    "\n",
    "    Balanced Dataset: — Let’s take a simple example if in our data set we have positive values which are approximately same as negative values. Then we can say our dataset in balance.\n",
    "    \n",
    "    Imbalanced Dataset: — If there is the very high different between the positive values and negative values. Then we can say our dataset in Imbalance Dataset.\n",
    "\n",
    "<br>\n",
    "<img src = \"images/balanced vs imbalanced.png\" width = 500 height = 500/> \n",
    "<br>\n",
    "\n",
    "<b> * How to work around imbalanced data issue :- </b>\n",
    "    \n",
    "    i) Under Sampling :- Undersampling says by using sampling tricks to create a new dataset such that, copy all the minority class points and randomly sample same amout of points from majority class in new dataset.\n",
    "    \n",
    "    Under-sampling, on contrary to over-sampling, aims to reduce the number of majority samples to balance the class distribution. Since it is removing observations from the original data set, it might discard useful information.\n",
    "    Advantages :-\n",
    "        Run-time can be improved by decreasing the amount of training dataset.\n",
    "        Helps in solving the memory problems.\n",
    "    Disadvantages :-\n",
    "        Losing some critical information\n",
    "    \n",
    "    ii) Over Sampling :- In oversampling we keep our majority class points, and we will create same amount of minority class points by replicating all minority points.\n",
    "    \n",
    "    Over-sampling increases the number of minority class members in the training set. The advantage of over-sampling is that no information from the original training set is lost, as all observations from the minority and majority classes are kept. On the other hand, it is prone to over fitting.\n",
    "    Advantages :-\n",
    "        No loss of information\n",
    "        Mitigate overfitting caused by oversampling.\n",
    "    Disadvantages :-\n",
    "        Overfitting\n",
    "<br>\n",
    "<img src = \"images/oversampling and undersampling.jpeg\" width = 500 height = 500/> \n",
    "<br>\n",
    "\n",
    "    iii) Class Weights :- In this method we give different weights to both the majority and minority classes. The difference in weights will influence the classification of the classes during the training phase. The whole purpose is to penalize the misclassification made by the minority class by setting a higher class weight and at the same time reducing weight for the majority class.\n",
    "    \n",
    "<br>\n",
    "<img src = \"images/Arjun-Singh_2-1598961758786.png\" width = 500 height = 500/> \n",
    "<br>\n",
    "\n",
    "    Class weights can be calculated as :- \n",
    "    \n",
    "$$\n",
    "    w_j=\\frac{n_{samples}}{(n_{classes} * n_{samples_{j}})}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text { where, }\\\\\n",
    "w_j \\text{ - is the weight for each class(j signifies the class), }\\\\\n",
    "n_{samples} \\text{ - is the total number of samples or rows in the dataset, }\\\\\n",
    "n_{classes} \\text{ - is the total number of unique classes in the target, }\\\\\n",
    "n_{samples_{j}} \\text{ - is the total number of rows of the respective class, }\\\\\n",
    "$\n",
    "\n",
    "    For example :- Lets qassume we have a dataset with such sample. n_samples=  43400,  n_classes= 2(0&1), n_sample0= 42617, n_samples1= 783.\n",
    "    \n",
    "    Solution :-\n",
    "        Weights for class 0:\n",
    "\n",
    "        w0=  43400/(2*42617) = 0.509\n",
    "\n",
    "        Weights for class 1:\n",
    "\n",
    "        w1= 43400/(2*783) = 27.713\n",
    "\n",
    "\n",
    "### Multiclass Classification :- \n",
    "    \n",
    "    A classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-class classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.\n",
    "    \n",
    "    Algorithms that are designed for binary classification can be adapted for use for multi-class problems.\n",
    "\n",
    "    This involves using a strategy of fitting multiple binary classification models for each class vs. all other classes (called one-vs-rest) or one model for each pair of classes (called one-vs-one).\n",
    "\n",
    "    One-vs-Rest: \n",
    "        Fit one binary classification model for each class vs. all other classes. Simply take one class as label 1 and other classes as 0.\n",
    "    One-vs-One: \n",
    "        Fit one binary classification model for each pair of classes. Simply if there are total 5 classes then build 5𝑐2 models for each pair. \n",
    "    Binary classification algorithms that can use these strategies for multi-class classification include:\n",
    "        Logistic Regression.\n",
    "        Support Vector Machine.\n",
    "<br>\n",
    "<img src = \"images/ovo-ovr.webp\"  width = 800 height = 800/> \n",
    "<br>\n",
    "\n",
    "### Multilabel Classification :- \n",
    "\n",
    "    Multi-label classification refers to those classification tasks that have two or more class labels, where one or more class labels may be predicted for each example.\n",
    "    Consider the example of photo classification, where a given photo may have multiple objects in the scene and a model may predict the presence of multiple known objects in the photo, such as “bicycle,” “apple,” “person,” etc.\n",
    "    Classification algorithms used for binary or multi-class classification cannot be used directly for multi-label classification. Specialized versions of standard classification algorithms can be used, so-called multi-label versions of the algorithms, including:\n",
    "        Multi-label Decision Trees\n",
    "        Multi-label Random Forests\n",
    "        Multi-label Gradient Boosting\n",
    "    \n",
    "    Another approach is to use a separate classification algorithm to predict the labels for each class.\n",
    "    \n",
    "<br>\n",
    "<img src = \"images/multilabel.webp\"  /> \n",
    "<br>\n",
    "\n",
    "### Local Outlier Factor :- \n",
    "    \n",
    "    Local outlier factor (LOF) is an algorithm that identifies the outliers present in the dataset. When a point is considered as an outlier based on its local neighborhood, it is a local outlier. LOF will identify an outlier considering the density of the neighborhood. LOF performs well when the density of the data is not the same throughout the dataset.\n",
    "\n",
    "    To understand LOF, we have to learn a few concepts sequentially:\n",
    "        i) K-distance and K-neighbors :- K-distance is the distance between the point, and it’s Kᵗʰ nearest neighbor. K-neighbors denoted by Nₖ(A) includes a set of points that lie in or on the circle of radius K-distance. \n",
    " \n",
    "<br>\n",
    "<img src = \"images/1_kOwduufhfK0yWWd3N4r5rQ.png\"  /> \n",
    "<$$ \\text{K-distance of A with K=2} $$\n",
    "<br>\n",
    "        \n",
    "        ii) Reachability distance (RD) :- It is defined as the maximum of K-distance of Xj and the distance between Xi and Xj. The distance measure is problem-specific (Euclidean, Manhattan, etc.)\n",
    "$$ RD(X_i,X_j) = max(K-distance(X_j),distance(X_i,X_j).  $$\n",
    "\n",
    "<br>\n",
    "<img src = \"images/1_zrk0QWDm7SXq2Iu69YnSFQ.png\"  /> \n",
    "<$$ \\text{Illustration of reachability distance with K=2} $$\n",
    "<br>\n",
    "\n",
    "    if a point Xi lies within the K-neighbors of Xj, the reachability distance will be K-distance of Xj (blue line), else reachability distance will be the distance between Xi and Xj (orange line).\n",
    "        \n",
    "        iii) Local reachability density (LRD) :- LRD is inverse of the average reachability distance of A from its neighbors. Intuitively according to LRD formula, more the average reachability distance (i.e., neighbors are far from the point), less density of points are present around a particular point. \n",
    "        This tells how far a point is from the nearest cluster of points. Low values of LRD implies that the closest cluster is far from the point.\n",
    "$$ LRD_k(A) = \\frac{1}{\\sum\\limits _{X_j \\in N_k(A)} \\frac{RD(A,X_j)}{||N_k(A)||}} \\\\\n",
    "||N_k(A)||  \\text{number of points in neighborhood of A.}\n",
    "$$\n",
    "        \n",
    "        iv) Local Outlier Factor (LOF) :- LRD of each point is used to compare with the average LRD of its K neighbors. LOF is the ratio of the average LRD of the K neighbors of A to the LRD of A.\n",
    "\n",
    "$$ LOF_k(A) = \\frac{\\sum\\limits _{X_j \\in N_k(A)}LRD_k(X_j)}{||N_k(A)||} * \\frac{1}{LRD_k(A)} \\\\\n",
    "$$\n",
    "$||N_k(A)||  \\text{number of points in neighborhood of A.}$\n",
    "        \n",
    "        Intuitively, if the point is not an outlier (inlier), the ratio of average LRD of neighbors is approximately equal to the LRD of a point (because the density of a point and its neighbors are roughly equal). In that case, LOF is nearly equal to 1. On the other hand, if the point is an outlier, the LRD of a point is less than the average LRD of neighbors. Then LOF value will be high.\n",
    "        Generally, if LOF> 1, it is considered as an outlier.\n",
    "        \n",
    "<b> Example :- 4 points: A(0,0), B(1,0), C(1,1) and D(0,3) and K=2. We will use LOF to detect one outlier among these 4 points. </b>\n",
    "<br>\n",
    "<img src = \"images/1_YSXLLWozQ5h0IoVN2F4IOw.png\"  /> \n",
    "<br>\n",
    "\n",
    "<b>Solution :- </b> First, calculate the K-distance, distance between each pair of points, and K-neighborhood of all the points with K=2. We will be using Manhattan distance as a measure of distance.\n",
    "\n",
    "K-distance(A) –> since C is the 2ᴺᴰ nearest neighbor of A –> distance(A, C) =2\n",
    "K-distance(B) –> since A, C are the 2ᴺᴰ nearest neighbor of B –> distance(B,C) OR distance(B,A) = 1\n",
    "K-distance(C) –> since A is the 2ᴺᴰ nearest neighbor of C –> distance(C,A) =2\n",
    "K-distance(D) –> since A,C are the 2ᴺᴰ nearest neighbor of D –> distance(D,A) or distance(D,C) =3\n",
    "<br>\n",
    "<img src = \"images/1_NygDyHNIDZjiU-2PUnieQQ.png\"  /> \n",
    "<br>\n",
    "\n",
    "K-neighborhood (A) = {B,C} , ||N2(A)|| =2<br>\n",
    "K-neighborhood (B) = {A,C}, ||N2(B)|| =2<br>\n",
    "K-neighborhood (C)= {B,A}, ||N2(C)|| =2<br>\n",
    "K-neighborhood (D) = {A,C}, ||N2(D)|| =2<br>\n",
    "\n",
    "K-distance, the distance between each pair of points, and K-neighborhood will be used to calculate LRD.\n",
    "<br>\n",
    "<img src = \"images/1_I-j2pTnIf2FAwhyA0llMew.png\"  /> \n",
    "<$$ \\text{LRD for each point A, B, C, and D} $$\n",
    "<br>\n",
    "\n",
    "Local reachability density (LRD) will be used to calculate the Local Outlier Factor (LOF).\n",
    "<br>\n",
    "<img src = \"images/1_gOONbPWNC7FMcODIpyVgFA.png\"  /> \n",
    "<$$ \\text{LRD for each point A, B, C, and D} $$\n",
    "<br>\n",
    "Highest LOF among the four points is LOF(D). Therefore, D is an outlier.\n",
    "\n",
    "<b>ADVANTAGES OF LOF :- </b>\n",
    "\n",
    "    A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. The global approach may not consider that point as an outlier. But the LOF can effectively identify the local outliers.\n",
    "    \n",
    "<b>DISADVANTAGES OF LOF :- </b>\n",
    "\n",
    "    Since LOF is a ratio, it is tough to interpret. There is no specific threshold value above which a point is defined as an outlier. The identification of an outlier is dependent on the problem and the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb06cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
